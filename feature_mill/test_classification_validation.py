#!/usr/bin/env python3
"""
æ€§åˆ«åˆ†ç±»å®éªŒéªŒè¯è„šæœ¬

è¿™ä¸ªè„šæœ¬ç”¨äºéªŒè¯æ€§åˆ«åˆ†ç±»å®éªŒçš„æ­£ç¡®æ€§å’Œå®Œæ•´æ€§ï¼š
1. æ£€æŸ¥è¾“å‡ºæ–‡ä»¶çš„å®Œæ•´æ€§
2. éªŒè¯æ¨¡å‹æ€§èƒ½æŒ‡æ ‡çš„åˆç†æ€§
3. æ£€æŸ¥ç‰¹å¾é‡è¦æ€§åˆ†æ
4. éªŒè¯æ··æ·†çŸ©é˜µå’Œåˆ†ç±»æŠ¥å‘Š
5. ä¸æœºå™¨å­¦ä¹ æœ€ä½³å®è·µå¯¹æ¯”
"""

import os
import sys
import pandas as pd
import numpy as np
import json
import logging
from datetime import datetime
from typing import Dict, List, Optional

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from feature_mill.experiment_engine import run_experiment
from feature_mill.experiment_result_manager import ExperimentResultManager

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


def validate_classification_results(output_dir: str) -> dict:
    """
    éªŒè¯åˆ†ç±»å®éªŒç»“æœ
    
    Args:
        output_dir: å®éªŒç»“æœè¾“å‡ºç›®å½•
    
    Returns:
        dict: éªŒè¯ç»“æœ
    """
    validation_results = {
        'model_comparison_valid': False,
        'individual_models_valid': False,
        'feature_importance_valid': False,
        'visualization_valid': False,
        'overall_valid': False,
        'issues': []
    }
    
    try:
        # 1. æ£€æŸ¥æ¨¡å‹æ¯”è¾ƒæ–‡ä»¶
        comparison_file = os.path.join(output_dir, 'model_comparison.csv')
        if os.path.exists(comparison_file):
            comparison_df = pd.read_csv(comparison_file)
            logger.info(f"æ¨¡å‹æ¯”è¾ƒæ–‡ä»¶åŒ…å« {len(comparison_df)} ä¸ªæ¨¡å‹")
            
            if len(comparison_df) > 0:
                # æ£€æŸ¥æ€§èƒ½æŒ‡æ ‡æ˜¯å¦åˆç†
                if 'accuracy' in comparison_df.columns:
                    accuracies = comparison_df['accuracy'].dropna()
                    if len(accuracies) > 0:
                        # æ£€æŸ¥å‡†ç¡®ç‡æ˜¯å¦åœ¨åˆç†èŒƒå›´å†…
                        if accuracies.max() <= 1.0 and accuracies.min() >= 0.0:
                            validation_results['model_comparison_valid'] = True
                        else:
                            validation_results['issues'].append("æ¨¡å‹å‡†ç¡®ç‡è¶…å‡ºåˆç†èŒƒå›´")
                    else:
                        validation_results['issues'].append("æ¨¡å‹æ¯”è¾ƒæ•°æ®ä¸ºç©º")
                else:
                    validation_results['issues'].append("æ¨¡å‹æ¯”è¾ƒæ–‡ä»¶ç¼ºå°‘accuracyåˆ—")
            else:
                validation_results['issues'].append("æ¨¡å‹æ¯”è¾ƒæ–‡ä»¶ä¸ºç©º")
        else:
            validation_results['issues'].append("æ¨¡å‹æ¯”è¾ƒæ–‡ä»¶ä¸å­˜åœ¨")
        
        # 2. æ£€æŸ¥å„ä¸ªæ¨¡å‹çš„å•ç‹¬ç»“æœæ–‡ä»¶
        model_names = ['Logistic_Regression', 'Random_Forest', 'Gradient_Boosting', 'SVM']
        required_files = [
            'classification_report_{}.csv',
            'confusion_matrix_{}.csv'
        ]
        
        valid_model_files = 0
        for model_name in model_names:
            model_valid = True
            for file_pattern in required_files:
                file_path = os.path.join(output_dir, file_pattern.format(model_name))
                if os.path.exists(file_path):
                    try:
                        model_df = pd.read_csv(file_path)
                        if len(model_df) > 0:
                            logger.info(f"æ¨¡å‹æ–‡ä»¶ {file_pattern.format(model_name)} åŒ…å« {len(model_df)} è¡Œæ•°æ®")
                        else:
                            validation_results['issues'].append(f"æ¨¡å‹æ–‡ä»¶ {file_pattern.format(model_name)} ä¸ºç©º")
                            model_valid = False
                    except Exception as e:
                        validation_results['issues'].append(f"è¯»å–æ¨¡å‹æ–‡ä»¶ {file_pattern.format(model_name)} å¤±è´¥: {str(e)}")
                        model_valid = False
                else:
                    validation_results['issues'].append(f"æ¨¡å‹æ–‡ä»¶ {file_pattern.format(model_name)} ä¸å­˜åœ¨")
                    model_valid = False
            
            if model_valid:
                valid_model_files += 1
        
        # è‡³å°‘åº”è¯¥æœ‰3ä¸ªæ¨¡å‹æ–‡ä»¶æœ‰æ•ˆ
        if valid_model_files >= 3:
            validation_results['individual_models_valid'] = True
        else:
            validation_results['issues'].append(f"æœ‰æ•ˆçš„æ¨¡å‹æ–‡ä»¶æ•°é‡ä¸è¶³: {valid_model_files}/4")
        
        # 3. æ£€æŸ¥ç‰¹å¾é‡è¦æ€§åˆ†æ
        importance_files = [
            'feature_importance_Logistic_Regression.csv',
            'feature_importance_Random_Forest.csv',
            'feature_importance_Gradient_Boosting.csv'
        ]
        
        valid_importance_files = 0
        for importance_file in importance_files:
            file_path = os.path.join(output_dir, importance_file)
            if os.path.exists(file_path):
                try:
                    importance_df = pd.read_csv(file_path)
                    if len(importance_df) > 0 and 'importance' in importance_df.columns:
                        valid_importance_files += 1
                        logger.info(f"ç‰¹å¾é‡è¦æ€§æ–‡ä»¶ {importance_file} åŒ…å« {len(importance_df)} ä¸ªç‰¹å¾")
                    else:
                        validation_results['issues'].append(f"ç‰¹å¾é‡è¦æ€§æ–‡ä»¶ {importance_file} æ•°æ®æ— æ•ˆ")
                except Exception as e:
                    validation_results['issues'].append(f"è¯»å–ç‰¹å¾é‡è¦æ€§æ–‡ä»¶ {importance_file} å¤±è´¥: {str(e)}")
            else:
                validation_results['issues'].append(f"ç‰¹å¾é‡è¦æ€§æ–‡ä»¶ {importance_file} ä¸å­˜åœ¨")
        
        # è‡³å°‘åº”è¯¥æœ‰2ä¸ªç‰¹å¾é‡è¦æ€§æ–‡ä»¶æœ‰æ•ˆ
        if valid_importance_files >= 2:
            validation_results['feature_importance_valid'] = True
        else:
            validation_results['issues'].append(f"æœ‰æ•ˆçš„ç‰¹å¾é‡è¦æ€§æ–‡ä»¶æ•°é‡ä¸è¶³: {valid_importance_files}/3")
        
        # 4. æ£€æŸ¥å¯è§†åŒ–æ–‡ä»¶
        visualization_files = [
            'model_comparison_plots.png',
            'confusion_matrices.png',
            'feature_importance_classification.png'
        ]
        
        valid_visualization_files = 0
        for viz_file in visualization_files:
            file_path = os.path.join(output_dir, viz_file)
            if os.path.exists(file_path):
                # æ£€æŸ¥æ–‡ä»¶å¤§å°æ˜¯å¦åˆç†ï¼ˆè‡³å°‘1KBï¼‰
                file_size = os.path.getsize(file_path)
                if file_size > 1024:
                    valid_visualization_files += 1
                    logger.info(f"å¯è§†åŒ–æ–‡ä»¶ {viz_file} å¤§å°: {file_size} bytes")
                else:
                    validation_results['issues'].append(f"å¯è§†åŒ–æ–‡ä»¶ {viz_file} å¤§å°å¼‚å¸¸: {file_size} bytes")
            else:
                validation_results['issues'].append(f"å¯è§†åŒ–æ–‡ä»¶ {viz_file} ä¸å­˜åœ¨")
        
        # è‡³å°‘åº”è¯¥æœ‰2ä¸ªå¯è§†åŒ–æ–‡ä»¶æœ‰æ•ˆ
        if valid_visualization_files >= 2:
            validation_results['visualization_valid'] = True
        else:
            validation_results['issues'].append(f"æœ‰æ•ˆçš„å¯è§†åŒ–æ–‡ä»¶æ•°é‡ä¸è¶³: {valid_visualization_files}/3")
        
        # 5. æ€»ä½“éªŒè¯
        if (validation_results['model_comparison_valid'] and 
            validation_results['individual_models_valid'] and
            validation_results['feature_importance_valid'] and
            validation_results['visualization_valid']):
            validation_results['overall_valid'] = True
        
    except Exception as e:
        validation_results['issues'].append(f"éªŒè¯è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}")
        logger.error(f"éªŒè¯è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
    
    return validation_results


def compare_with_ml_best_practices(results_dir: str) -> dict:
    """
    ä¸æœºå™¨å­¦ä¹ æœ€ä½³å®è·µè¿›è¡Œå¯¹æ¯”
    
    Args:
        results_dir: å®éªŒç»“æœç›®å½•
    
    Returns:
        dict: å¯¹æ¯”ç»“æœ
    """
    comparison_results = {
        'model_diversity': False,
        'performance_reasonable': False,
        'feature_importance_consistent': False,
        'cross_validation_used': False,
        'overall_best_practices': False,
        'findings': []
    }
    
    try:
        # 1. æ£€æŸ¥æ¨¡å‹å¤šæ ·æ€§
        comparison_file = os.path.join(results_dir, 'model_comparison.csv')
        if os.path.exists(comparison_file):
            comparison_df = pd.read_csv(comparison_file)
            if len(comparison_df) >= 3:  # è‡³å°‘åº”è¯¥æœ‰3ç§ä¸åŒç±»å‹çš„æ¨¡å‹
                comparison_results['model_diversity'] = True
                comparison_results['findings'].append(f"ä½¿ç”¨äº† {len(comparison_df)} ç§ä¸åŒç±»å‹çš„æ¨¡å‹ï¼Œæ¨¡å‹å¤šæ ·æ€§è‰¯å¥½")
        
        # 2. æ£€æŸ¥æ€§èƒ½æ˜¯å¦åˆç†
        if os.path.exists(comparison_file):
            comparison_df = pd.read_csv(comparison_file)
            if 'accuracy' in comparison_df.columns:
                accuracies = comparison_df['accuracy'].dropna()
                if len(accuracies) > 0:
                    # æ£€æŸ¥å‡†ç¡®ç‡æ˜¯å¦åœ¨åˆç†èŒƒå›´å†…ï¼ˆå¯¹äºæ€§åˆ«åˆ†ç±»ï¼Œé€šå¸¸0.6-0.9æ˜¯åˆç†çš„ï¼‰
                    if accuracies.max() <= 0.95 and accuracies.min() >= 0.5:
                        comparison_results['performance_reasonable'] = True
                        comparison_results['findings'].append(f"æ€§åˆ«åˆ†ç±»æ¨¡å‹å‡†ç¡®ç‡åœ¨åˆç†èŒƒå›´å†…: {accuracies.min():.3f}-{accuracies.max():.3f}")
                    else:
                        comparison_results['findings'].append(f"æ€§åˆ«åˆ†ç±»æ¨¡å‹å‡†ç¡®ç‡å¯èƒ½å¼‚å¸¸: {accuracies.min():.3f}-{accuracies.max():.3f}")
        
        # 3. æ£€æŸ¥ç‰¹å¾é‡è¦æ€§ä¸€è‡´æ€§
        importance_files = [
            'feature_importance_Random_Forest.csv',
            'feature_importance_Gradient_Boosting.csv'
        ]
        
        importance_consistency = True
        for importance_file in importance_files:
            file_path = os.path.join(results_dir, importance_file)
            if os.path.exists(file_path):
                try:
                    importance_df = pd.read_csv(file_path)
                    if len(importance_df) > 0 and 'importance' in importance_df.columns:
                        # æ£€æŸ¥æ˜¯å¦æœ‰éé›¶é‡è¦æ€§ç‰¹å¾
                        non_zero_importance = importance_df[importance_df['importance'] > 0]
                        if len(non_zero_importance) > 0:
                            comparison_results['findings'].append(f"{importance_file} åŒ…å« {len(non_zero_importance)} ä¸ªéé›¶é‡è¦æ€§ç‰¹å¾")
                        else:
                            importance_consistency = False
                            comparison_results['findings'].append(f"{importance_file} æ²¡æœ‰éé›¶é‡è¦æ€§ç‰¹å¾")
                except Exception as e:
                    importance_consistency = False
                    comparison_results['findings'].append(f"æ£€æŸ¥ç‰¹å¾é‡è¦æ€§æ–‡ä»¶å¤±è´¥: {str(e)}")
        
        if importance_consistency:
            comparison_results['feature_importance_consistent'] = True
        
        # 4. æ£€æŸ¥æ˜¯å¦ä½¿ç”¨äº†äº¤å‰éªŒè¯
        # è¿™é€šå¸¸å¯ä»¥ä»æ¨¡å‹æ¯”è¾ƒæ–‡ä»¶ä¸­çš„æ ‡å‡†å·®åˆ—æ¨æ–­
        if os.path.exists(comparison_file):
            comparison_df = pd.read_csv(comparison_file)
            if 'std_accuracy' in comparison_df.columns or 'cv_score' in comparison_df.columns:
                comparison_results['cross_validation_used'] = True
                comparison_results['findings'].append("ä½¿ç”¨äº†äº¤å‰éªŒè¯è¯„ä¼°æ¨¡å‹æ€§èƒ½")
        
        # 5. æ€»ä½“æœ€ä½³å®è·µè¯„ä¼°
        if (comparison_results['model_diversity'] and 
            comparison_results['performance_reasonable'] and
            comparison_results['feature_importance_consistent'] and
            comparison_results['cross_validation_used']):
            comparison_results['overall_best_practices'] = True
            comparison_results['findings'].append("æ€»ä½“ç¬¦åˆæœºå™¨å­¦ä¹ æœ€ä½³å®è·µ")
        
    except Exception as e:
        comparison_results['findings'].append(f"å¯¹æ¯”è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}")
        logger.error(f"å¯¹æ¯”è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
    
    return comparison_results


def analyze_classification_performance(results_dir: str) -> dict:
    """
    åˆ†æåˆ†ç±»æ€§èƒ½
    
    Args:
        results_dir: å®éªŒç»“æœç›®å½•
    
    Returns:
        dict: æ€§èƒ½åˆ†æç»“æœ
    """
    performance_analysis = {
        'best_model': None,
        'model_rankings': [],
        'performance_insights': [],
        'findings': []
    }
    
    try:
        # è¯»å–æ¨¡å‹æ¯”è¾ƒç»“æœ
        comparison_file = os.path.join(results_dir, 'model_comparison.csv')
        if os.path.exists(comparison_file):
            comparison_df = pd.read_csv(comparison_file)
            
            # æ‰¾å‡ºæœ€ä½³æ¨¡å‹
            if 'accuracy' in comparison_df.columns:
                best_idx = comparison_df['accuracy'].idxmax()
                best_model = comparison_df.loc[best_idx, 'model']
                performance_analysis['best_model'] = best_model
                performance_analysis['findings'].append(f"æœ€ä½³æ¨¡å‹: {best_model}")
            
            # æ¨¡å‹æ’å
            if 'accuracy' in comparison_df.columns:
                ranked_models = comparison_df.sort_values('accuracy', ascending=False)
                performance_analysis['model_rankings'] = ranked_models['model'].tolist()
                performance_analysis['findings'].append(f"æ¨¡å‹æ’å: {', '.join(ranked_models['model'].tolist())}")
                
                # æ€§èƒ½æ´å¯Ÿ
                accuracy_range = ranked_models['accuracy'].max() - ranked_models['accuracy'].min()
                if accuracy_range < 0.1:
                    performance_analysis['performance_insights'].append("æ¨¡å‹æ€§èƒ½å·®å¼‚è¾ƒå°ï¼Œå¯èƒ½éœ€è¦æ›´å¤šç‰¹å¾å·¥ç¨‹")
                elif accuracy_range > 0.3:
                    performance_analysis['performance_insights'].append("æ¨¡å‹æ€§èƒ½å·®å¼‚è¾ƒå¤§ï¼ŒæŸäº›æ¨¡å‹å¯èƒ½ä¸é€‚åˆè¯¥æ•°æ®é›†")
                else:
                    performance_analysis['performance_insights'].append("æ¨¡å‹æ€§èƒ½å·®å¼‚é€‚ä¸­")
        
        # åˆ†æç‰¹å¾é‡è¦æ€§
        importance_files = [
            'feature_importance_Random_Forest.csv',
            'feature_importance_Gradient_Boosting.csv'
        ]
        
        for importance_file in importance_files:
            file_path = os.path.join(results_dir, importance_file)
            if os.path.exists(file_path):
                try:
                    importance_df = pd.read_csv(file_path)
                    if len(importance_df) > 0 and 'importance' in importance_df.columns:
                        top_features = importance_df.nlargest(5, 'importance')
                        model_name = importance_file.replace('feature_importance_', '').replace('.csv', '')
                        performance_analysis['findings'].append(f"{model_name} å‰5é‡è¦ç‰¹å¾: {', '.join(top_features['feature'].tolist())}")
                except Exception as e:
                    performance_analysis['findings'].append(f"åˆ†æç‰¹å¾é‡è¦æ€§æ–‡ä»¶å¤±è´¥: {str(e)}")
        
    except Exception as e:
        performance_analysis['findings'].append(f"æ€§èƒ½åˆ†æè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}")
        logger.error(f"æ€§èƒ½åˆ†æè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
    
    return performance_analysis


def run_classification_test():
    """è¿è¡Œåˆ†ç±»å®éªŒæµ‹è¯•"""
    print("ğŸ”¬ å¼€å§‹æ€§åˆ«åˆ†ç±»å®éªŒéªŒè¯æµ‹è¯•")
    print("=" * 60)
    
    # å®éªŒå‚æ•°
    dataset_id = 1  # minimal_harvardæ•°æ®é›†
    feature_set_id = 1
    output_dir = "data/experiments/classification_validation"
    
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    os.makedirs(output_dir, exist_ok=True)
    
    try:
        # 1. è¿è¡Œåˆ†ç±»å®éªŒ
        print("ğŸ“Š æ­¥éª¤1: è¿è¡Œæ€§åˆ«åˆ†ç±»å®éªŒ...")
        start_time = datetime.now()
        
        result = run_experiment(
            experiment_type='classification',
            dataset_id=dataset_id,
            feature_set_id=feature_set_id,
            output_dir=output_dir,
            extra_args={
                'target_var': 'sex',  # æ”¹ä¸ºæ€§åˆ«åˆ†ç±»
                'test_size': 0.2,
                'random_state': 42,
                'n_splits': 5,
                'plot_results': True,
                'plot_feature_importance': True
            }
        )
        
        end_time = datetime.now()
        duration = (end_time - start_time).total_seconds()
        
        print(f"âœ… æ€§åˆ«åˆ†ç±»å®éªŒå®Œæˆï¼Œè€—æ—¶: {duration:.2f}ç§’")
        print(f"ğŸ“ ç»“æœä¿å­˜åœ¨: {output_dir}")
        
        # 2. éªŒè¯å®éªŒç»“æœ
        print("\nğŸ” æ­¥éª¤2: éªŒè¯å®éªŒç»“æœ...")
        validation_results = validate_classification_results(output_dir)
        
        print("éªŒè¯ç»“æœ:")
        for key, value in validation_results.items():
            if key != 'issues':
                status = "âœ… é€šè¿‡" if value else "âŒ å¤±è´¥"
                print(f"  {key}: {status}")
        
        if validation_results['issues']:
            print("\nâš ï¸ å‘ç°çš„é—®é¢˜:")
            for issue in validation_results['issues']:
                print(f"  - {issue}")
        
        # 3. ä¸æœºå™¨å­¦ä¹ æœ€ä½³å®è·µå¯¹æ¯”
        print("\nğŸ“š æ­¥éª¤3: ä¸æœºå™¨å­¦ä¹ æœ€ä½³å®è·µå¯¹æ¯”...")
        comparison_results = compare_with_ml_best_practices(output_dir)
        
        print("æœ€ä½³å®è·µå¯¹æ¯”ç»“æœ:")
        for key, value in comparison_results.items():
            if key != 'findings':
                status = "âœ… ç¬¦åˆ" if value else "âŒ ä¸ç¬¦åˆ"
                print(f"  {key}: {status}")
        
        if comparison_results['findings']:
            print("\nğŸ“– å‘ç°:")
            for finding in comparison_results['findings']:
                print(f"  - {finding}")
        
        # 4. åˆ†æåˆ†ç±»æ€§èƒ½
        print("\nğŸ“ˆ æ­¥éª¤4: åˆ†æåˆ†ç±»æ€§èƒ½...")
        performance_analysis = analyze_classification_performance(output_dir)
        
        if performance_analysis['findings']:
            print("æ€§èƒ½åˆ†æç»“æœ:")
            for finding in performance_analysis['findings']:
                print(f"  - {finding}")
        
        # 5. ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š
        print("\nğŸ“‹ æ­¥éª¤5: ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š...")
        generate_test_report(validation_results, comparison_results, performance_analysis, result, output_dir)
        
        # 6. æ€»ä½“è¯„ä¼°
        print("\nğŸ¯ æ€»ä½“è¯„ä¼°:")
        if validation_results['overall_valid'] and comparison_results['overall_best_practices']:
            print("âœ… æ€§åˆ«åˆ†ç±»å®éªŒéªŒè¯æˆåŠŸï¼ç¬¦åˆæœºå™¨å­¦ä¹ æœ€ä½³å®è·µã€‚")
        else:
            print("âš ï¸ æ€§åˆ«åˆ†ç±»å®éªŒéªŒè¯éƒ¨åˆ†æˆåŠŸï¼Œéœ€è¦è¿›ä¸€æ­¥æ£€æŸ¥ã€‚")
        
        return {
            'success': validation_results['overall_valid'] and comparison_results['overall_best_practices'],
            'validation_results': validation_results,
            'comparison_results': comparison_results,
            'performance_analysis': performance_analysis,
            'experiment_result': result
        }
        
    except Exception as e:
        print(f"âŒ æ€§åˆ«åˆ†ç±»å®éªŒæµ‹è¯•å¤±è´¥: {e}")
        logger.error(f"æ€§åˆ«åˆ†ç±»å®éªŒæµ‹è¯•å¤±è´¥: {e}")
        return {'success': False, 'error': str(e)}


def generate_test_report(validation_results, comparison_results, performance_analysis, experiment_result, output_dir):
    """ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š"""
    report_file = os.path.join(output_dir, 'validation_report.txt')
    
    with open(report_file, 'w', encoding='utf-8') as f:
        f.write("æ€§åˆ«åˆ†ç±»å®éªŒéªŒè¯æŠ¥å‘Š\n")
        f.write("=" * 50 + "\n\n")
        
        f.write("1. å®éªŒåŸºæœ¬ä¿¡æ¯\n")
        f.write("-" * 20 + "\n")
        f.write(f"å®éªŒç±»å‹: classification\n")
        f.write(f"æ•°æ®é›†ID: 1 \n")
        f.write(f"ç‰¹å¾é›†ID: 1\n")
        f.write(f"ç›®æ ‡å˜é‡: sex\n")
        f.write(f"å®éªŒID: {experiment_result.get('experiment_result_id', 'N/A')}\n")
        f.write(f"è¿è¡Œæ—¶é—´: {experiment_result.get('duration', 'N/A')}ç§’\n\n")
        
        f.write("2. éªŒè¯ç»“æœ\n")
        f.write("-" * 20 + "\n")
        for key, value in validation_results.items():
            if key != 'issues':
                status = "é€šè¿‡" if value else "å¤±è´¥"
                f.write(f"{key}: {status}\n")
        
        if validation_results['issues']:
            f.write("\nå‘ç°çš„é—®é¢˜:\n")
            for issue in validation_results['issues']:
                f.write(f"- {issue}\n")
        
        f.write("\n3. æœ€ä½³å®è·µå¯¹æ¯”ç»“æœ\n")
        f.write("-" * 20 + "\n")
        for key, value in comparison_results.items():
            if key != 'findings':
                status = "ç¬¦åˆ" if value else "ä¸ç¬¦åˆ"
                f.write(f"{key}: {status}\n")
        
        if comparison_results['findings']:
            f.write("\nå‘ç°:\n")
            for finding in comparison_results['findings']:
                f.write(f"- {finding}\n")
        
        f.write("\n4. æ€§èƒ½åˆ†æç»“æœ\n")
        f.write("-" * 20 + "\n")
        if performance_analysis['findings']:
            for finding in performance_analysis['findings']:
                f.write(f"- {finding}\n")
        
        f.write("\n5. æ€»ä½“è¯„ä¼°\n")
        f.write("-" * 20 + "\n")
        if validation_results['overall_valid'] and comparison_results['overall_best_practices']:
            f.write("âœ… æ€§åˆ«åˆ†ç±»å®éªŒéªŒè¯æˆåŠŸï¼ç¬¦åˆæœºå™¨å­¦ä¹ æœ€ä½³å®è·µã€‚\n")
        else:
            f.write("âš ï¸ æ€§åˆ«åˆ†ç±»å®éªŒéªŒè¯éƒ¨åˆ†æˆåŠŸï¼Œéœ€è¦è¿›ä¸€æ­¥æ£€æŸ¥ã€‚\n")
    
    print(f"ğŸ“„ éªŒè¯æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")


if __name__ == "__main__":
    run_classification_test() 