#!/usr/bin/env python3
"""
ç‰¹å¾é€‰æ‹©å®éªŒéªŒè¯è„šæœ¬

è¿™ä¸ªè„šæœ¬ç”¨äºï¼š
1. è¿è¡Œç‰¹å¾é€‰æ‹©å®éªŒ
2. éªŒè¯å®éªŒç»“æœçš„æ­£ç¡®æ€§
3. ä¸å·²çŸ¥çš„EEGç ”ç©¶ç»“æœè¿›è¡Œå¯¹æ¯”
4. æ£€æŸ¥ç‰¹å¾é€‰æ‹©æ–¹æ³•çš„åˆç†æ€§
"""

import os
import sys
import pandas as pd
import numpy as np
from datetime import datetime
import logging

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from feature_mill.experiment_engine import run_experiment

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


def validate_feature_selection_results(output_dir: str) -> dict:
    """
    éªŒè¯ç‰¹å¾é€‰æ‹©å®éªŒç»“æœ
    
    Args:
        output_dir: å®éªŒç»“æœè¾“å‡ºç›®å½•
    
    Returns:
        dict: éªŒè¯ç»“æœ
    """
    validation_results = {
        'selection_summary_valid': False,
        'methods_comparison_valid': False,
        'individual_methods_valid': False,
        'feature_importance_valid': False,
        'overall_valid': False,
        'issues': []
    }
    
    try:
        # 1. æ£€æŸ¥ç‰¹å¾é€‰æ‹©æ±‡æ€»æ–‡ä»¶
        summary_file = os.path.join(output_dir, 'feature_selection_summary.csv')
        if os.path.exists(summary_file):
            summary_df = pd.read_csv(summary_file)
            logger.info(f"ç‰¹å¾é€‰æ‹©æ±‡æ€»æ–‡ä»¶åŒ…å« {len(summary_df)} ä¸ªç‰¹å¾")
            
            if len(summary_df) > 0:
                # æ£€æŸ¥æ–¹æ³•ä½¿ç”¨æ¬¡æ•°æ˜¯å¦åˆç†
                methods_using = summary_df['methods_using'].dropna()
                n_methods = summary_df['n_methods'].dropna()
                
                if len(methods_using) > 0 and len(n_methods) > 0:
                    # æ£€æŸ¥æ–¹æ³•ä½¿ç”¨æ¬¡æ•°æ˜¯å¦åœ¨åˆç†èŒƒå›´å†…
                    if n_methods.max() <= 7 and n_methods.min() >= 0:  # æœ€å¤š7ç§æ–¹æ³•
                        validation_results['selection_summary_valid'] = True
                    else:
                        validation_results['issues'].append("æ–¹æ³•ä½¿ç”¨æ¬¡æ•°è¶…å‡ºåˆç†èŒƒå›´")
                else:
                    validation_results['issues'].append("ç‰¹å¾é€‰æ‹©æ±‡æ€»æ•°æ®ä¸ºç©º")
            else:
                validation_results['issues'].append("ç‰¹å¾é€‰æ‹©æ±‡æ€»æ–‡ä»¶ä¸ºç©º")
        else:
            validation_results['issues'].append("ç‰¹å¾é€‰æ‹©æ±‡æ€»æ–‡ä»¶ä¸å­˜åœ¨")
        
        # 2. æ£€æŸ¥æ–¹æ³•æ¯”è¾ƒæ–‡ä»¶
        comparison_file = os.path.join(output_dir, 'selection_methods_comparison.csv')
        if os.path.exists(comparison_file):
            comparison_df = pd.read_csv(comparison_file)
            comparison_df.columns = [col.strip() for col in comparison_df.columns]  # å»é™¤åˆ—åç©ºæ ¼
            if len(comparison_df) > 0:
                if 'mean_cv_score' in comparison_df.columns:
                    cv_scores = comparison_df['mean_cv_score'].dropna()
                    if len(cv_scores) > 0:
                        if cv_scores.max() <= 1.0 and cv_scores.min() >= -10.0:
                            validation_results['methods_comparison_valid'] = True
                        else:
                            validation_results['issues'].append("äº¤å‰éªŒè¯åˆ†æ•°è¶…å‡ºåˆç†èŒƒå›´")
                    else:
                        validation_results['issues'].append("æ–¹æ³•æ¯”è¾ƒæ•°æ®ä¸ºç©º")
                else:
                    validation_results['issues'].append("æ–¹æ³•æ¯”è¾ƒæ–‡ä»¶ç¼ºå°‘mean_cv_scoreåˆ—")
            else:
                validation_results['issues'].append("æ–¹æ³•æ¯”è¾ƒæ–‡ä»¶ä¸ºç©º")
        else:
            validation_results['issues'].append("æ–¹æ³•æ¯”è¾ƒæ–‡ä»¶ä¸å­˜åœ¨")
        
        # 3. æ£€æŸ¥å„ä¸ªæ–¹æ³•çš„å•ç‹¬ç»“æœæ–‡ä»¶
        method_files = [
            'selection_variance.csv',
            'selection_correlation.csv',
            'selection_univariate_f.csv',
            'selection_mutual_info.csv',
            'selection_lasso.csv',
            'selection_rfe.csv',
            'selection_pca.csv'
        ]
        
        valid_method_files = 0
        for method_file in method_files:
            file_path = os.path.join(output_dir, method_file)
            if os.path.exists(file_path):
                try:
                    method_df = pd.read_csv(file_path)
                    if len(method_df) > 0:
                        valid_method_files += 1
                        logger.info(f"æ–¹æ³•æ–‡ä»¶ {method_file} åŒ…å« {len(method_df)} ä¸ªç‰¹å¾")
                    else:
                        validation_results['issues'].append(f"æ–¹æ³•æ–‡ä»¶ {method_file} ä¸ºç©º")
                except Exception as e:
                    validation_results['issues'].append(f"è¯»å–æ–¹æ³•æ–‡ä»¶ {method_file} å¤±è´¥: {str(e)}")
            else:
                validation_results['issues'].append(f"æ–¹æ³•æ–‡ä»¶ {method_file} ä¸å­˜åœ¨")
        
        # è‡³å°‘åº”è¯¥æœ‰4ä¸ªæ–¹æ³•æ–‡ä»¶æœ‰æ•ˆ
        if valid_method_files >= 4:
            validation_results['individual_methods_valid'] = True
        else:
            validation_results['issues'].append(f"æœ‰æ•ˆçš„æ–¹æ³•æ–‡ä»¶æ•°é‡ä¸è¶³: {valid_method_files}/7")
        
        # 4. æ£€æŸ¥ç‰¹å¾é‡è¦æ€§åˆ†æ
        importance_file = os.path.join(output_dir, 'feature_importance_analysis.png')
        if os.path.exists(importance_file):
            validation_results['feature_importance_valid'] = True
        else:
            validation_results['issues'].append("ç‰¹å¾é‡è¦æ€§åˆ†æå›¾ä¸å­˜åœ¨")
        
        # 5. æ€»ä½“éªŒè¯
        if (validation_results['selection_summary_valid'] and 
            validation_results['methods_comparison_valid'] and
            validation_results['individual_methods_valid'] and
            validation_results['feature_importance_valid']):
            validation_results['overall_valid'] = True
        
    except Exception as e:
        validation_results['issues'].append(f"éªŒè¯è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}")
        logger.error(f"éªŒè¯è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
    
    return validation_results


def compare_with_eeg_literature(results_dir: str) -> dict:
    """
    ä¸EEGæ–‡çŒ®ä¸­çš„å·²çŸ¥ç»“æœè¿›è¡Œå¯¹æ¯”
    
    Args:
        results_dir: å®éªŒç»“æœç›®å½•
    
    Returns:
        dict: å¯¹æ¯”ç»“æœ
    """
    comparison_results = {
        'alpha_power_selection': False,
        'entropy_selection': False,
        'peak_frequency_selection': False,
        'method_diversity': False,
        'overall_consistency': False,
        'findings': []
    }
    
    try:
        # è¯»å–ç‰¹å¾é€‰æ‹©æ±‡æ€»ç»“æœ
        summary_file = os.path.join(results_dir, 'feature_selection_summary.csv')
        if not os.path.exists(summary_file):
            comparison_results['findings'].append("æ— æ³•æ‰¾åˆ°ç‰¹å¾é€‰æ‹©æ±‡æ€»æ–‡ä»¶")
            return comparison_results
        
        summary_df = pd.read_csv(summary_file)
        
        # 1. æ£€æŸ¥Î±æ³¢åŠŸç‡ç‰¹å¾çš„é€‰æ‹©æƒ…å†µ
        alpha_features = summary_df[summary_df['feature'].str.contains('alpha|bp_alpha', case=False)]
        if len(alpha_features) > 0:
            logger.info(f"å‘ç° {len(alpha_features)} ä¸ªÎ±æ³¢ç›¸å…³ç‰¹å¾è¢«é€‰æ‹©")
            comparison_results['findings'].append(f"å‘ç° {len(alpha_features)} ä¸ªÎ±æ³¢ç›¸å…³ç‰¹å¾è¢«é€‰æ‹©")
            
            # æ£€æŸ¥Î±æ³¢ç‰¹å¾æ˜¯å¦è¢«å¤šç§æ–¹æ³•é€‰æ‹©
            high_selection_alpha = alpha_features[alpha_features['n_methods'] >= 3]
            if len(high_selection_alpha) > 0:
                comparison_results['alpha_power_selection'] = True
                comparison_results['findings'].append("Î±æ³¢åŠŸç‡ç‰¹å¾è¢«å¤šç§æ–¹æ³•é€‰æ‹©ï¼Œç¬¦åˆEEGç ”ç©¶æ–‡çŒ®")
        
        # 2. æ£€æŸ¥ç†µç‰¹å¾çš„é€‰æ‹©æƒ…å†µ
        entropy_features = summary_df[summary_df['feature'].str.contains('entropy', case=False)]
        if len(entropy_features) > 0:
            logger.info(f"å‘ç° {len(entropy_features)} ä¸ªç†µç›¸å…³ç‰¹å¾è¢«é€‰æ‹©")
            comparison_results['findings'].append(f"å‘ç° {len(entropy_features)} ä¸ªç†µç›¸å…³ç‰¹å¾è¢«é€‰æ‹©")
            
            # æ£€æŸ¥ç†µç‰¹å¾æ˜¯å¦è¢«å¤šç§æ–¹æ³•é€‰æ‹©
            high_selection_entropy = entropy_features[entropy_features['n_methods'] >= 3]
            if len(high_selection_entropy) > 0:
                comparison_results['entropy_selection'] = True
                comparison_results['findings'].append("ç†µç‰¹å¾è¢«å¤šç§æ–¹æ³•é€‰æ‹©ï¼Œç¬¦åˆEEGå¤æ‚åº¦åˆ†ææ–‡çŒ®")
        
        # 3. æ£€æŸ¥å³°å€¼é¢‘ç‡ç‰¹å¾çš„é€‰æ‹©æƒ…å†µ
        peak_features = summary_df[summary_df['feature'].str.contains('peak', case=False)]
        if len(peak_features) > 0:
            logger.info(f"å‘ç° {len(peak_features)} ä¸ªå³°å€¼é¢‘ç‡ç›¸å…³ç‰¹å¾è¢«é€‰æ‹©")
            comparison_results['findings'].append(f"å‘ç° {len(peak_features)} ä¸ªå³°å€¼é¢‘ç‡ç›¸å…³ç‰¹å¾è¢«é€‰æ‹©")
            
            if len(peak_features) > 0:
                comparison_results['peak_frequency_selection'] = True
                comparison_results['findings'].append("å³°å€¼é¢‘ç‡ç‰¹å¾è¢«é€‰æ‹©ï¼Œç¬¦åˆEEGé¢‘è°±åˆ†ææ–‡çŒ®")
        
        # 4. æ£€æŸ¥æ–¹æ³•å¤šæ ·æ€§
        methods_comparison_file = os.path.join(results_dir, 'selection_methods_comparison.csv')
        if os.path.exists(methods_comparison_file):
            methods_df = pd.read_csv(methods_comparison_file)
            if len(methods_df) >= 5:  # è‡³å°‘åº”è¯¥æœ‰5ç§æ–¹æ³•
                comparison_results['method_diversity'] = True
                comparison_results['findings'].append(f"ä½¿ç”¨äº† {len(methods_df)} ç§ç‰¹å¾é€‰æ‹©æ–¹æ³•ï¼Œæ–¹æ³•å¤šæ ·æ€§è‰¯å¥½")
        
        # 5. æ€»ä½“ä¸€è‡´æ€§è¯„ä¼°
        if (comparison_results['alpha_power_selection'] and 
            comparison_results['entropy_selection'] and
            comparison_results['peak_frequency_selection'] and
            comparison_results['method_diversity']):
            comparison_results['overall_consistency'] = True
            comparison_results['findings'].append("æ€»ä½“ç»“æœä¸EEGç ”ç©¶æ–‡çŒ®é«˜åº¦ä¸€è‡´")
        
    except Exception as e:
        comparison_results['findings'].append(f"å¯¹æ¯”è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}")
        logger.error(f"å¯¹æ¯”è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
    
    return comparison_results


def analyze_selection_methods_performance(results_dir: str) -> dict:
    """
    åˆ†æç‰¹å¾é€‰æ‹©æ–¹æ³•çš„æ€§èƒ½
    
    Args:
        results_dir: å®éªŒç»“æœç›®å½•
    
    Returns:
        dict: æ€§èƒ½åˆ†æç»“æœ
    """
    performance_analysis = {
        'best_method': None,
        'method_rankings': [],
        'feature_overlap': {},
        'findings': []
    }
    
    try:
        # è¯»å–æ–¹æ³•æ¯”è¾ƒç»“æœ
        comparison_file = os.path.join(results_dir, 'selection_methods_comparison.csv')
        if os.path.exists(comparison_file):
            comparison_df = pd.read_csv(comparison_file)
            
            # æ‰¾å‡ºæœ€ä½³æ–¹æ³•
            if 'mean_cv_score' in comparison_df.columns:
                best_idx = comparison_df['mean_cv_score'].idxmax()
                best_method = best_idx  # ç´¢å¼•å°±æ˜¯æ–¹æ³•åç§°
                performance_analysis['best_method'] = best_method
                performance_analysis['findings'].append(f"æœ€ä½³æ–¹æ³•: {best_method}")
            
            # æ–¹æ³•æ’å
            if 'mean_cv_score' in comparison_df.columns:
                ranked_methods = comparison_df.sort_values('mean_cv_score', ascending=False)
                performance_analysis['method_rankings'] = ranked_methods.index.tolist()
                performance_analysis['findings'].append(f"æ–¹æ³•æ’å: {', '.join(ranked_methods.index.tolist())}")
        
        # åˆ†æç‰¹å¾é‡å 
        summary_file = os.path.join(results_dir, 'feature_selection_summary.csv')
        if os.path.exists(summary_file):
            summary_df = pd.read_csv(summary_file)
            
            # æ‰¾å‡ºè¢«å¤šç§æ–¹æ³•é€‰æ‹©çš„ç‰¹å¾
            high_selection_features = summary_df[summary_df['n_methods'] >= 3]
            if len(high_selection_features) > 0:
                performance_analysis['feature_overlap'] = {
                    'high_selection_features': high_selection_features['feature'].tolist(),
                    'count': len(high_selection_features)
                }
                performance_analysis['findings'].append(f"è¢«3ç§ä»¥ä¸Šæ–¹æ³•é€‰æ‹©çš„ç‰¹å¾: {len(high_selection_features)} ä¸ª")
        
    except Exception as e:
        performance_analysis['findings'].append(f"æ€§èƒ½åˆ†æè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}")
        logger.error(f"æ€§èƒ½åˆ†æè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
    
    return performance_analysis


def run_feature_selection_test():
    """è¿è¡Œç‰¹å¾é€‰æ‹©å®éªŒæµ‹è¯•"""
    print("ğŸ”¬ å¼€å§‹ç‰¹å¾é€‰æ‹©å®éªŒéªŒè¯æµ‹è¯•")
    print("=" * 60)
    
    # å®éªŒå‚æ•°
    dataset_id = 1  # minimal_harvardæ•°æ®é›†
    feature_set_id = 1
    output_dir = "data/experiments/feature_selection_validation"
    
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    os.makedirs(output_dir, exist_ok=True)
    
    try:
        # 1. è¿è¡Œç‰¹å¾é€‰æ‹©å®éªŒ
        print("ğŸ“Š æ­¥éª¤1: è¿è¡Œç‰¹å¾é€‰æ‹©å®éªŒ...")
        start_time = datetime.now()
        
        result = run_experiment(
            experiment_type='feature_selection',
            dataset_id=dataset_id,
            feature_set_id=feature_set_id,
            output_dir=output_dir,
            extra_args={
                'target_var': 'age',
                'n_features': 20,
                'variance_threshold': 0.01,
                'correlation_threshold': 0.95,
                'plot_selection_results': True,
                'plot_feature_importance': True
            }
        )
        
        end_time = datetime.now()
        duration = (end_time - start_time).total_seconds()
        
        print(f"âœ… ç‰¹å¾é€‰æ‹©å®éªŒå®Œæˆï¼Œè€—æ—¶: {duration:.2f}ç§’")
        print(f"ğŸ“ ç»“æœä¿å­˜åœ¨: {output_dir}")
        
        # 2. éªŒè¯å®éªŒç»“æœ
        print("\nğŸ” æ­¥éª¤2: éªŒè¯å®éªŒç»“æœ...")
        validation_results = validate_feature_selection_results(output_dir)
        
        print("éªŒè¯ç»“æœ:")
        for key, value in validation_results.items():
            if key != 'issues':
                status = "âœ… é€šè¿‡" if value else "âŒ å¤±è´¥"
                print(f"  {key}: {status}")
        
        if validation_results['issues']:
            print("\nâš ï¸ å‘ç°çš„é—®é¢˜:")
            for issue in validation_results['issues']:
                print(f"  - {issue}")
        
        # 3. ä¸EEGæ–‡çŒ®å¯¹æ¯”
        print("\nğŸ“š æ­¥éª¤3: ä¸EEGç ”ç©¶æ–‡çŒ®å¯¹æ¯”...")
        comparison_results = compare_with_eeg_literature(output_dir)
        
        print("æ–‡çŒ®å¯¹æ¯”ç»“æœ:")
        for key, value in comparison_results.items():
            if key != 'findings':
                status = "âœ… ä¸€è‡´" if value else "âŒ ä¸ä¸€è‡´"
                print(f"  {key}: {status}")
        
        if comparison_results['findings']:
            print("\nğŸ“– å‘ç°:")
            for finding in comparison_results['findings']:
                print(f"  - {finding}")
        
        # 4. åˆ†æé€‰æ‹©æ–¹æ³•æ€§èƒ½
        print("\nğŸ“ˆ æ­¥éª¤4: åˆ†æç‰¹å¾é€‰æ‹©æ–¹æ³•æ€§èƒ½...")
        performance_analysis = analyze_selection_methods_performance(output_dir)
        
        if performance_analysis['findings']:
            print("æ€§èƒ½åˆ†æç»“æœ:")
            for finding in performance_analysis['findings']:
                print(f"  - {finding}")
        
        # 5. ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š
        print("\nğŸ“‹ æ­¥éª¤5: ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š...")
        generate_test_report(validation_results, comparison_results, performance_analysis, result, output_dir)
        
        # 6. æ€»ä½“è¯„ä¼°
        print("\nğŸ¯ æ€»ä½“è¯„ä¼°:")
        if validation_results['overall_valid'] and comparison_results['overall_consistency']:
            print("âœ… ç‰¹å¾é€‰æ‹©å®éªŒéªŒè¯æˆåŠŸï¼ç»“æœä¸EEGç ”ç©¶æ–‡çŒ®ä¸€è‡´ã€‚")
        else:
            print("âš ï¸ ç‰¹å¾é€‰æ‹©å®éªŒéªŒè¯éƒ¨åˆ†æˆåŠŸï¼Œéœ€è¦è¿›ä¸€æ­¥æ£€æŸ¥ã€‚")
        
        return {
            'success': validation_results['overall_valid'] and comparison_results['overall_consistency'],
            'validation_results': validation_results,
            'comparison_results': comparison_results,
            'performance_analysis': performance_analysis,
            'experiment_result': result
        }
        
    except Exception as e:
        print(f"âŒ ç‰¹å¾é€‰æ‹©å®éªŒæµ‹è¯•å¤±è´¥: {e}")
        logger.error(f"ç‰¹å¾é€‰æ‹©å®éªŒæµ‹è¯•å¤±è´¥: {e}")
        return {'success': False, 'error': str(e)}


def generate_test_report(validation_results, comparison_results, performance_analysis, experiment_result, output_dir):
    """ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š"""
    report_file = os.path.join(output_dir, 'validation_report.txt')
    
    with open(report_file, 'w', encoding='utf-8') as f:
        f.write("ç‰¹å¾é€‰æ‹©å®éªŒéªŒè¯æŠ¥å‘Š\n")
        f.write("=" * 50 + "\n\n")
        
        f.write("1. å®éªŒåŸºæœ¬ä¿¡æ¯\n")
        f.write("-" * 20 + "\n")
        f.write(f"å®éªŒç±»å‹: feature_selection\n")
        f.write(f"æ•°æ®é›†ID: 3 (minimal_harvard)\n")
        f.write(f"ç‰¹å¾é›†ID: 1\n")
        f.write(f"ç›®æ ‡å˜é‡: age\n")
        f.write(f"å®éªŒID: {experiment_result.get('experiment_result_id', 'N/A')}\n")
        f.write(f"è¿è¡Œæ—¶é—´: {experiment_result.get('duration', 'N/A')}ç§’\n\n")
        
        f.write("2. éªŒè¯ç»“æœ\n")
        f.write("-" * 20 + "\n")
        for key, value in validation_results.items():
            if key != 'issues':
                status = "é€šè¿‡" if value else "å¤±è´¥"
                f.write(f"{key}: {status}\n")
        
        if validation_results['issues']:
            f.write("\nå‘ç°çš„é—®é¢˜:\n")
            for issue in validation_results['issues']:
                f.write(f"- {issue}\n")
        
        f.write("\n3. æ–‡çŒ®å¯¹æ¯”ç»“æœ\n")
        f.write("-" * 20 + "\n")
        for key, value in comparison_results.items():
            if key != 'findings':
                status = "ä¸€è‡´" if value else "ä¸ä¸€è‡´"
                f.write(f"{key}: {status}\n")
        
        if comparison_results['findings']:
            f.write("\nå‘ç°:\n")
            for finding in comparison_results['findings']:
                f.write(f"- {finding}\n")
        
        f.write("\n4. æ€§èƒ½åˆ†æç»“æœ\n")
        f.write("-" * 20 + "\n")
        if performance_analysis['best_method']:
            f.write(f"æœ€ä½³æ–¹æ³•: {performance_analysis['best_method']}\n")
        
        if performance_analysis['method_rankings']:
            f.write(f"æ–¹æ³•æ’å: {', '.join(performance_analysis['method_rankings'])}\n")
        
        if performance_analysis['feature_overlap']:
            overlap_info = performance_analysis['feature_overlap']
            f.write(f"é«˜é€‰æ‹©ç‰¹å¾æ•°é‡: {overlap_info['count']}\n")
        
        if performance_analysis['findings']:
            f.write("\næ€§èƒ½åˆ†æå‘ç°:\n")
            for finding in performance_analysis['findings']:
                f.write(f"- {finding}\n")
        
        f.write("\n5. æ€»ä½“è¯„ä¼°\n")
        f.write("-" * 20 + "\n")
        if validation_results['overall_valid'] and comparison_results['overall_consistency']:
            f.write("âœ… ç‰¹å¾é€‰æ‹©å®éªŒéªŒè¯æˆåŠŸï¼ç»“æœä¸EEGç ”ç©¶æ–‡çŒ®ä¸€è‡´ã€‚\n")
        else:
            f.write("âš ï¸ ç‰¹å¾é€‰æ‹©å®éªŒéªŒè¯éƒ¨åˆ†æˆåŠŸï¼Œéœ€è¦è¿›ä¸€æ­¥æ£€æŸ¥ã€‚\n")
    
    print(f"ğŸ“„ æµ‹è¯•æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")


if __name__ == "__main__":
    run_feature_selection_test() 