#!/usr/bin/env python3
"""
ç‰¹å¾ç»Ÿè®¡å®éªŒéªŒè¯è„šæœ¬

è¿™ä¸ªè„šæœ¬ç”¨äºï¼š
1. è¿è¡Œç‰¹å¾ç»Ÿè®¡å®éªŒ
2. éªŒè¯å®éªŒç»“æœçš„æ­£ç¡®æ€§
3. ä¸å·²çŸ¥çš„EEGç ”ç©¶ç»“æœè¿›è¡Œå¯¹æ¯”
4. æ£€æŸ¥ç‰¹å¾æå–çš„åˆç†æ€§
"""

import os
import sys
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
import logging

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from feature_mill.experiment_engine import run_experiment
from feature_mill.experiment_result_manager import ExperimentResultManager

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# è®¾ç½®matplotlibä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False


def validate_feature_statistics_results(output_dir: str) -> dict:
    """
    éªŒè¯ç‰¹å¾ç»Ÿè®¡å®éªŒç»“æœ
    
    Args:
        output_dir: å®éªŒç»“æœè¾“å‡ºç›®å½•
    
    Returns:
        dict: éªŒè¯ç»“æœ
    """
    validation_results = {
        'basic_stats_valid': False,
        'distribution_analysis_valid': False,
        'outlier_analysis_valid': False,
        'feature_importance_valid': False,
        'overall_valid': False,
        'issues': []
    }
    
    try:
        # 1. æ£€æŸ¥åŸºæœ¬ç»Ÿè®¡æ–‡ä»¶
        basic_stats_file = os.path.join(output_dir, 'feature_basic_statistics.csv')
        if os.path.exists(basic_stats_file):
            basic_stats_df = pd.read_csv(basic_stats_file)
            logger.info(f"åŸºæœ¬ç»Ÿè®¡æ–‡ä»¶åŒ…å« {len(basic_stats_df)} ä¸ªç‰¹å¾")
            
            # éªŒè¯åŸºæœ¬ç»Ÿè®¡çš„åˆç†æ€§
            if len(basic_stats_df) > 0:
                # æ£€æŸ¥ç»Ÿè®¡å€¼æ˜¯å¦åœ¨åˆç†èŒƒå›´å†…
                mean_values = basic_stats_df['mean'].dropna()
                std_values = basic_stats_df['std'].dropna()
                
                if len(mean_values) > 0 and len(std_values) > 0:
                    # æ£€æŸ¥æ˜¯å¦æœ‰å¼‚å¸¸å€¼
                    if not (mean_values.isin([np.inf, -np.inf]).any() or 
                           std_values.isin([np.inf, -np.inf]).any()):
                        validation_results['basic_stats_valid'] = True
                    else:
                        validation_results['issues'].append("åŸºæœ¬ç»Ÿè®¡ä¸­å­˜åœ¨æ— ç©·å€¼")
                else:
                    validation_results['issues'].append("åŸºæœ¬ç»Ÿè®¡æ•°æ®ä¸ºç©º")
            else:
                validation_results['issues'].append("åŸºæœ¬ç»Ÿè®¡æ–‡ä»¶ä¸ºç©º")
        else:
            validation_results['issues'].append("åŸºæœ¬ç»Ÿè®¡æ–‡ä»¶ä¸å­˜åœ¨")
        
        # 2. æ£€æŸ¥åˆ†å¸ƒåˆ†ææ–‡ä»¶
        distribution_file = os.path.join(output_dir, 'feature_distribution_analysis.csv')
        if os.path.exists(distribution_file):
            distribution_df = pd.read_csv(distribution_file)
            logger.info(f"åˆ†å¸ƒåˆ†ææ–‡ä»¶åŒ…å« {len(distribution_df)} ä¸ªç‰¹å¾")
            
            if len(distribution_df) > 0:
                # æ£€æŸ¥åˆ†å¸ƒç±»å‹æ˜¯å¦åˆç†
                dist_types = distribution_df['distribution_type'].value_counts()
                logger.info(f"åˆ†å¸ƒç±»å‹åˆ†å¸ƒ: {dist_types.to_dict()}")
                
                # éªŒè¯ååº¦å’Œå³°åº¦å€¼
                skewness_values = distribution_df['skewness'].dropna()
                kurtosis_values = distribution_df['kurtosis'].dropna()
                
                if len(skewness_values) > 0 and len(kurtosis_values) > 0:
                    if not (skewness_values.isin([np.inf, -np.inf]).any() or 
                           kurtosis_values.isin([np.inf, -np.inf]).any()):
                        validation_results['distribution_analysis_valid'] = True
                    else:
                        validation_results['issues'].append("åˆ†å¸ƒåˆ†æä¸­å­˜åœ¨æ— ç©·å€¼")
                else:
                    validation_results['issues'].append("åˆ†å¸ƒåˆ†ææ•°æ®ä¸ºç©º")
            else:
                validation_results['issues'].append("åˆ†å¸ƒåˆ†ææ–‡ä»¶ä¸ºç©º")
        else:
            validation_results['issues'].append("åˆ†å¸ƒåˆ†ææ–‡ä»¶ä¸å­˜åœ¨")
        
        # 3. æ£€æŸ¥å¼‚å¸¸å€¼åˆ†ææ–‡ä»¶
        outlier_file = os.path.join(output_dir, 'feature_outlier_analysis.csv')
        if os.path.exists(outlier_file):
            outlier_df = pd.read_csv(outlier_file)
            logger.info(f"å¼‚å¸¸å€¼åˆ†ææ–‡ä»¶åŒ…å« {len(outlier_df)} ä¸ªç‰¹å¾")
            
            if len(outlier_df) > 0:
                # æ£€æŸ¥å¼‚å¸¸å€¼æ¯”ä¾‹æ˜¯å¦åˆç†
                outlier_percentages = outlier_df['outlier_percentage'].dropna()
                if len(outlier_percentages) > 0:
                    if outlier_percentages.max() <= 100 and outlier_percentages.min() >= 0:
                        validation_results['outlier_analysis_valid'] = True
                    else:
                        validation_results['issues'].append("å¼‚å¸¸å€¼æ¯”ä¾‹è¶…å‡ºåˆç†èŒƒå›´")
                else:
                    validation_results['issues'].append("å¼‚å¸¸å€¼åˆ†ææ•°æ®ä¸ºç©º")
            else:
                validation_results['issues'].append("å¼‚å¸¸å€¼åˆ†ææ–‡ä»¶ä¸ºç©º")
        else:
            validation_results['issues'].append("å¼‚å¸¸å€¼åˆ†ææ–‡ä»¶ä¸å­˜åœ¨")
        
        # 4. æ£€æŸ¥ç‰¹å¾é‡è¦æ€§æ–‡ä»¶
        importance_file = os.path.join(output_dir, 'feature_importance_ranking.csv')
        if os.path.exists(importance_file):
            importance_df = pd.read_csv(importance_file)
            logger.info(f"ç‰¹å¾é‡è¦æ€§æ–‡ä»¶åŒ…å« {len(importance_df)} ä¸ªç‰¹å¾")
            
            if len(importance_df) > 0:
                # æ£€æŸ¥é‡è¦æ€§åˆ†æ•°æ˜¯å¦åˆç†
                importance_scores = importance_df['importance_score'].dropna()
                if len(importance_scores) > 0:
                    if importance_scores.max() >= 0 and importance_scores.min() >= 0:
                        validation_results['feature_importance_valid'] = True
                    else:
                        validation_results['issues'].append("ç‰¹å¾é‡è¦æ€§åˆ†æ•°åŒ…å«è´Ÿå€¼")
                else:
                    validation_results['issues'].append("ç‰¹å¾é‡è¦æ€§æ•°æ®ä¸ºç©º")
            else:
                validation_results['issues'].append("ç‰¹å¾é‡è¦æ€§æ–‡ä»¶ä¸ºç©º")
        else:
            validation_results['issues'].append("ç‰¹å¾é‡è¦æ€§æ–‡ä»¶ä¸å­˜åœ¨")
        
        # 5. æ€»ä½“éªŒè¯
        if (validation_results['basic_stats_valid'] and 
            validation_results['distribution_analysis_valid'] and
            validation_results['outlier_analysis_valid'] and
            validation_results['feature_importance_valid']):
            validation_results['overall_valid'] = True
        
    except Exception as e:
        validation_results['issues'].append(f"éªŒè¯è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}")
        logger.error(f"éªŒè¯è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
    
    return validation_results


def compare_with_eeg_literature(results_dir: str) -> dict:
    """
    ä¸EEGæ–‡çŒ®ä¸­çš„å·²çŸ¥ç»“æœè¿›è¡Œå¯¹æ¯”
    
    Args:
        results_dir: å®éªŒç»“æœç›®å½•
    
    Returns:
        dict: å¯¹æ¯”ç»“æœ
    """
    comparison_results = {
        'alpha_power_consistency': False,
        'entropy_consistency': False,
        'peak_frequency_consistency': False,
        'overall_consistency': False,
        'findings': []
    }
    
    try:
        # è¯»å–ç‰¹å¾é‡è¦æ€§ç»“æœ
        importance_file = os.path.join(results_dir, 'feature_importance_ranking.csv')
        if not os.path.exists(importance_file):
            comparison_results['findings'].append("æ— æ³•æ‰¾åˆ°ç‰¹å¾é‡è¦æ€§æ–‡ä»¶")
            return comparison_results
        
        importance_df = pd.read_csv(importance_file)
        
        # 1. æ£€æŸ¥Î±æ³¢åŠŸç‡ç‰¹å¾çš„ä¸€è‡´æ€§
        alpha_features = importance_df[importance_df['feature'].str.contains('alpha|bp_alpha', case=False)]
        if len(alpha_features) > 0:
            logger.info(f"å‘ç° {len(alpha_features)} ä¸ªÎ±æ³¢ç›¸å…³ç‰¹å¾")
            comparison_results['findings'].append(f"å‘ç° {len(alpha_features)} ä¸ªÎ±æ³¢ç›¸å…³ç‰¹å¾")
            
            # æ£€æŸ¥Î±æ³¢ç‰¹å¾æ˜¯å¦åœ¨é‡è¦ç‰¹å¾ä¸­
            top_alpha_features = alpha_features.head(10)
            if len(top_alpha_features) > 0:
                comparison_results['alpha_power_consistency'] = True
                comparison_results['findings'].append("Î±æ³¢åŠŸç‡ç‰¹å¾åœ¨é‡è¦ç‰¹å¾ä¸­è¡¨ç°è‰¯å¥½ï¼Œç¬¦åˆEEGç ”ç©¶æ–‡çŒ®")
        
        # 2. æ£€æŸ¥ç†µç‰¹å¾çš„ä¸€è‡´æ€§
        entropy_features = importance_df[importance_df['feature'].str.contains('entropy', case=False)]
        if len(entropy_features) > 0:
            logger.info(f"å‘ç° {len(entropy_features)} ä¸ªç†µç›¸å…³ç‰¹å¾")
            comparison_results['findings'].append(f"å‘ç° {len(entropy_features)} ä¸ªç†µç›¸å…³ç‰¹å¾")
            
            # æ£€æŸ¥ç†µç‰¹å¾æ˜¯å¦åœ¨é‡è¦ç‰¹å¾ä¸­
            top_entropy_features = entropy_features.head(10)
            if len(top_entropy_features) > 0:
                comparison_results['entropy_consistency'] = True
                comparison_results['findings'].append("ç†µç‰¹å¾åœ¨é‡è¦ç‰¹å¾ä¸­è¡¨ç°è‰¯å¥½ï¼Œç¬¦åˆEEGå¤æ‚åº¦åˆ†ææ–‡çŒ®")
        
        # 3. æ£€æŸ¥å³°å€¼é¢‘ç‡ç‰¹å¾çš„ä¸€è‡´æ€§
        peak_features = importance_df[importance_df['feature'].str.contains('peak', case=False)]
        if len(peak_features) > 0:
            logger.info(f"å‘ç° {len(peak_features)} ä¸ªå³°å€¼é¢‘ç‡ç›¸å…³ç‰¹å¾")
            comparison_results['findings'].append(f"å‘ç° {len(peak_features)} ä¸ªå³°å€¼é¢‘ç‡ç›¸å…³ç‰¹å¾")
            
            if len(peak_features) > 0:
                comparison_results['peak_frequency_consistency'] = True
                comparison_results['findings'].append("å³°å€¼é¢‘ç‡ç‰¹å¾å­˜åœ¨ï¼Œç¬¦åˆEEGé¢‘è°±åˆ†ææ–‡çŒ®")
        
        # 4. æ€»ä½“ä¸€è‡´æ€§è¯„ä¼°
        if (comparison_results['alpha_power_consistency'] and 
            comparison_results['entropy_consistency'] and
            comparison_results['peak_frequency_consistency']):
            comparison_results['overall_consistency'] = True
            comparison_results['findings'].append("æ€»ä½“ç»“æœä¸EEGç ”ç©¶æ–‡çŒ®é«˜åº¦ä¸€è‡´")
        
    except Exception as e:
        comparison_results['findings'].append(f"å¯¹æ¯”è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}")
        logger.error(f"å¯¹æ¯”è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
    
    return comparison_results


def run_feature_statistics_test():
    """è¿è¡Œç‰¹å¾ç»Ÿè®¡å®éªŒæµ‹è¯•"""
    print("ğŸ”¬ å¼€å§‹ç‰¹å¾ç»Ÿè®¡å®éªŒéªŒè¯æµ‹è¯•")
    print("=" * 60)
    
    # å®éªŒå‚æ•°
    dataset_id = 1  # minimal_harvardæ•°æ®é›†
    feature_set_id = 1
    output_dir = "data/experiments/feature_statistics_validation"
    
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    os.makedirs(output_dir, exist_ok=True)
    
    try:
        # 1. è¿è¡Œç‰¹å¾ç»Ÿè®¡å®éªŒ
        print("ğŸ“Š æ­¥éª¤1: è¿è¡Œç‰¹å¾ç»Ÿè®¡å®éªŒ...")
        start_time = datetime.now()
        
        result = run_experiment(
            experiment_type='feature_statistics',
            dataset_id=dataset_id,
            feature_set_id=feature_set_id,
            output_dir=output_dir,
            extra_args={
                'outlier_method': 'iqr',
                'outlier_threshold': 1.5,
                'plot_distributions': True,
                'plot_correlation_heatmap': True,
                'plot_outliers': True,
                'top_n_features': 20
            }
        )
        
        end_time = datetime.now()
        duration = (end_time - start_time).total_seconds()
        
        print(f"âœ… ç‰¹å¾ç»Ÿè®¡å®éªŒå®Œæˆï¼Œè€—æ—¶: {duration:.2f}ç§’")
        print(f"ğŸ“ ç»“æœä¿å­˜åœ¨: {output_dir}")
        
        # 2. éªŒè¯å®éªŒç»“æœ
        print("\nğŸ” æ­¥éª¤2: éªŒè¯å®éªŒç»“æœ...")
        validation_results = validate_feature_statistics_results(output_dir)
        
        print("éªŒè¯ç»“æœ:")
        for key, value in validation_results.items():
            if key != 'issues':
                status = "âœ… é€šè¿‡" if value else "âŒ å¤±è´¥"
                print(f"  {key}: {status}")
        
        if validation_results['issues']:
            print("\nâš ï¸ å‘ç°çš„é—®é¢˜:")
            for issue in validation_results['issues']:
                print(f"  - {issue}")
        
        # 3. ä¸EEGæ–‡çŒ®å¯¹æ¯”
        print("\nğŸ“š æ­¥éª¤3: ä¸EEGç ”ç©¶æ–‡çŒ®å¯¹æ¯”...")
        comparison_results = compare_with_eeg_literature(output_dir)
        
        print("æ–‡çŒ®å¯¹æ¯”ç»“æœ:")
        for key, value in comparison_results.items():
            if key != 'findings':
                status = "âœ… ä¸€è‡´" if value else "âŒ ä¸ä¸€è‡´"
                print(f"  {key}: {status}")
        
        if comparison_results['findings']:
            print("\nğŸ“– å‘ç°:")
            for finding in comparison_results['findings']:
                print(f"  - {finding}")
        
        # 4. ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š
        print("\nğŸ“‹ æ­¥éª¤4: ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š...")
        generate_test_report(validation_results, comparison_results, result, output_dir)
        
        # 5. æ€»ä½“è¯„ä¼°
        print("\nğŸ¯ æ€»ä½“è¯„ä¼°:")
        if validation_results['overall_valid'] and comparison_results['overall_consistency']:
            print("âœ… ç‰¹å¾ç»Ÿè®¡å®éªŒéªŒè¯æˆåŠŸï¼ç»“æœä¸EEGç ”ç©¶æ–‡çŒ®ä¸€è‡´ã€‚")
        else:
            print("âš ï¸ ç‰¹å¾ç»Ÿè®¡å®éªŒéªŒè¯éƒ¨åˆ†æˆåŠŸï¼Œéœ€è¦è¿›ä¸€æ­¥æ£€æŸ¥ã€‚")
        
        return {
            'success': validation_results['overall_valid'] and comparison_results['overall_consistency'],
            'validation_results': validation_results,
            'comparison_results': comparison_results,
            'experiment_result': result
        }
        
    except Exception as e:
        print(f"âŒ ç‰¹å¾ç»Ÿè®¡å®éªŒæµ‹è¯•å¤±è´¥: {e}")
        logger.error(f"ç‰¹å¾ç»Ÿè®¡å®éªŒæµ‹è¯•å¤±è´¥: {e}")
        return {'success': False, 'error': str(e)}


def generate_test_report(validation_results, comparison_results, experiment_result, output_dir):
    """ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š"""
    report_file = os.path.join(output_dir, 'validation_report.txt')
    
    with open(report_file, 'w', encoding='utf-8') as f:
        f.write("ç‰¹å¾ç»Ÿè®¡å®éªŒéªŒè¯æŠ¥å‘Š\n")
        f.write("=" * 50 + "\n\n")
        
        f.write("1. å®éªŒåŸºæœ¬ä¿¡æ¯\n")
        f.write("-" * 20 + "\n")
        f.write(f"å®éªŒç±»å‹: feature_statistics\n")
        f.write(f"æ•°æ®é›†ID: 3 (minimal_harvard)\n")
        f.write(f"ç‰¹å¾é›†ID: 1\n")
        f.write(f"å®éªŒID: {experiment_result.get('experiment_result_id', 'N/A')}\n")
        f.write(f"è¿è¡Œæ—¶é—´: {experiment_result.get('duration', 'N/A')}ç§’\n\n")
        
        f.write("2. éªŒè¯ç»“æœ\n")
        f.write("-" * 20 + "\n")
        for key, value in validation_results.items():
            if key != 'issues':
                status = "é€šè¿‡" if value else "å¤±è´¥"
                f.write(f"{key}: {status}\n")
        
        if validation_results['issues']:
            f.write("\nå‘ç°çš„é—®é¢˜:\n")
            for issue in validation_results['issues']:
                f.write(f"- {issue}\n")
        
        f.write("\n3. æ–‡çŒ®å¯¹æ¯”ç»“æœ\n")
        f.write("-" * 20 + "\n")
        for key, value in comparison_results.items():
            if key != 'findings':
                status = "ä¸€è‡´" if value else "ä¸ä¸€è‡´"
                f.write(f"{key}: {status}\n")
        
        if comparison_results['findings']:
            f.write("\nå‘ç°:\n")
            for finding in comparison_results['findings']:
                f.write(f"- {finding}\n")
        
        f.write("\n4. æ€»ä½“è¯„ä¼°\n")
        f.write("-" * 20 + "\n")
        if validation_results['overall_valid'] and comparison_results['overall_consistency']:
            f.write("âœ… ç‰¹å¾ç»Ÿè®¡å®éªŒéªŒè¯æˆåŠŸï¼ç»“æœä¸EEGç ”ç©¶æ–‡çŒ®ä¸€è‡´ã€‚\n")
        else:
            f.write("âš ï¸ ç‰¹å¾ç»Ÿè®¡å®éªŒéªŒè¯éƒ¨åˆ†æˆåŠŸï¼Œéœ€è¦è¿›ä¸€æ­¥æ£€æŸ¥ã€‚\n")
    
    print(f"ğŸ“„ æµ‹è¯•æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")


if __name__ == "__main__":
    run_feature_statistics_test() 